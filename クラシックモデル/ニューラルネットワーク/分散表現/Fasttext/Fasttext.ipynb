{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. FastTex [->Github](https://github.com/facebookresearch/fastText)\n",
    "\n",
    "    1.1 FastTextは何ですか。\n",
    "        \n",
    "    FastTextは、Word2Vecのアイデアを拡張して開発された、単語の埋め込みを生成するためのライブラリです。\n",
    "\n",
    "    1.2 word2vecとの違いはいくつあります。\n",
    "\n",
    "    １。サブワード情報の利用：FastTextは、単語をより小さな単位（サブワード、具体的にはn-gram）に分割し、これらのサブワードの埋め込みを学習します。*その後、単語の最終的なベクトル表現は、その構成サブワードのベクトルの合計または平均によって得られます*。このアプローチにより、未知の単語やレアワードに対しても表現を生成することができ、形態素の豊富な言語や新しい単語が頻繁に生成されるドメインでの性能が向上します。Word2Vecでは、単語全体を最小単位として扱い、単語レベルでのみベクトル表現を学習します。そのため、訓練データに存在しない単語のベクトルを生成することはできません。\n",
    "\n",
    "    2. 未知語やレアワードへの対応：FastTextは未知語やレアワードに強いです。サブワード情報を利用することで、これらの単語の意味を推測し、適切なベクトル表現を生成することができます。Word2Vecは、学習時に見た単語に対してのみベクトル表現を持っているため、未知語に対しては直接的な対応が困難です。\n",
    "\n",
    "    3. 計算コストとパフォーマンス：FastTextはサブワード情報を利用するため、Word2Vecに比べて訓練にはより多くの計算リソースを必要とすることがあります。しかし、その結果として得られる単語の埋め込みは、より情報豊富であり、特に形態素が豊富な言語や未知語、レアワードの処理において優れた性能を発揮します。Word2Vecは比較的シンプルで、計算コストはFastTextよりも低いですが、未知語やレアワードへの対応能力には限界があります。\n",
    "\n",
    "    1.3 FastTextソースコード解析（モデル）：\n",
    "    \n",
    "    [-> 論文の補充1: Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606v1.pdf)\n",
    "\n",
    "    => main points: Use the sub-word to represent the words can be a good solution to the oov.\n",
    "\n",
    "    => Experiments: Performance on Word Similarity and Analogy Tasks: They evaluated how well the word vectors generated by their method and the traditional methods performed on standard linguistic tasks. These tasks often involve measuring the similarity between words or solving word analogies (e.g., \"man\" is to \"woman\" as \"king\" is to \"queen\"). The goal was to see if the subword information could lead to better understanding and representation of words, especially for languages with rich morphology.\n",
    "\n",
    "    Ability to Handle Rare and Out-of-Vocabulary (OOV) Words: A significant part of the comparison was to assess how effectively each method could deal with rare words or words that were not present in the training data. Since the new method uses subword information, it was hypothesized to be better at handling such words by leveraging their morphological structure (e.g., prefixes, suffixes).\n",
    "\n",
    "    Performance Across Different Languages: The experiments were not limited to English but included several languages with varying morphological complexity. This comparison aimed to demonstrate the versatility and effectiveness of the subword information approach across different linguistic contexts.\n",
    "    \n",
    "    => other points:\n",
    "    - Word representations trained on large unlabeled corpora are useful for natural language processing tasks.\n",
    "    - Existing models for learning word representations ignore the morphology of words, which is a limitation for morphologically rich languages with large vocabularies and many rare words.\n",
    "    - The proposed model represents each word as a bag of character n-grams and associates a vector representation to each n-gram.\n",
    "    - The vector representation of a word is obtained by summing the representations of its n-grams.\n",
    "    - The proposed model is fast and allows for training on large corpora quickly.\n",
    "    - The authors evaluate the model on five different languages and show its benefits in capturing word similarity and analogy.\n",
    "    - The model outperforms baseline models in word similarity and analogy tasks.\n",
    "\n",
    "    [-> 論文の補充2: Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759v1.pdf)\n",
    "    \n",
    "    \n",
    "    1.4 Fasttextの実践\n",
    "    \n",
    "    [Yahoo!ニュースをクラスタリング](https://qiita.com/kei0919/items/3059c336c3d0e2228830)\n",
    "    \n",
    "    [fastText Japanese Tutorial](https://github.com/icoxfog417/fastTextJapaneseTutorial)\n",
    "\n",
    "    [FacebookのfastTextでFastに単語の分散表現を獲得する](https://qiita.com/icoxfog417/items/42a95b279c0b7ad26589)\n",
    "\n",
    "\n",
    "    1.5 FastTextの欠点はなんですか。\n",
    "\n",
    "    1. 計算資源の要求が高い\n",
    "    2. モデルサイズが大きい\n",
    "    3. 未知語への過剰適応\n",
    "    4. 言語による違い\n",
    "    5. コンテキストの無視\n",
    "\n",
    "    1.6 FastTextの利点はなんですか\n",
    "\n",
    "    1. サブワード情報の活用:FastTextは単語をサブワード（n-gram）に分割してベクトルを学習するため、未学習の単語や形態素が異なる単語でも意味をうまく捉えることができます。これにより、綴りの似た単語や派生語の意味を理解する能力が向上します。\n",
    "    \n",
    "    2. 効率的な学習と推論:FastTextは効率的に学習できるよう設計されており、特に大規模なコーパスに対しても高速に学習を行うことができます。モデルが軽量であり、リアルタイムアプリケーションでの推論にも適しています。\n",
    "\n",
    "    3. 高い汎化性能: サブワード情報を使うことで、辞書に含まれない単語や希少な単語に対してもモデルが適応しやすくなり、一般的な単語埋め込み技術（例：Word2Vec）よりも高い汎化性能を示します。\n",
    "    \n",
    "    4. 単語の形態を考慮した表現: FastTextは単語の形態的特徴を捉えることができるため、特に形態素が重要な役割を果たす言語（例：日本語やフィンランド語などの膠着語）において優れた性能を発揮します。\n",
    "\n",
    "    \n",
    "Japanese reference: \n",
    "![fasttext](./image/fasttext.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# トレーニング用のサンプルデータ（文章のリスト）\n",
    "sentences = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"barked\"],\n",
    "    [\"the\", \"cat\", \"meowed\"],\n",
    "    # ここにさらに文章を追加して、トレーニングデータを充実させることができます\n",
    "]\n",
    "\n",
    "# FastTextモデルの初期化とトレーニング\n",
    "# vector_size: 埋め込みベクトルの次元数\n",
    "# window: コンテキストのウィンドウサイズ\n",
    "# min_count: 最低出現回数\n",
    "# sg: 1はskip-gramを使用、0はCBOWを使用\n",
    "model = FastText(vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# 単語の語彙を構築\n",
    "model.build_vocab(sentences=sentences)\n",
    "\n",
    "# モデルをトレーニング\n",
    "# total_examples: トレーニングデータの総例数\n",
    "# epochs: トレーニングのエポック数\n",
    "model.train(sentences=sentences, total_examples=len(sentences), epochs=10)\n",
    "\n",
    "# 単語 \"cat\" の埋め込みベクトルを取得\n",
    "cat_vector = model.wv[\"cat\"]\n",
    "print(\"Cat vector:\", cat_vector)\n",
    "\n",
    "# モデルをファイルに保存\n",
    "model.save(\"fasttext.model\")\n",
    "\n",
    "# 保存されたモデルをロード\n",
    "loaded_model = FastText.load(\"fasttext.model\")\n",
    "\n",
    "# ロードされたモデルで再度単語 \"cat\" のベクトルを取得\n",
    "loaded_cat_vector = loaded_model.wv[\"cat\"]\n",
    "print(\"Loaded Cat vector:\", loaded_cat_vector)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
