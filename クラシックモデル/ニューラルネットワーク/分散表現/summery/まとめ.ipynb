{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. NLPで文書の表現方法：\n",
    "\n",
    "    **BoWモデルに基づく方法**：one-hot、TF-IDF、TextRankなど。これらの方法は、文書を単語の出現頻度ベクトルに変換します。\n",
    "    \n",
    "    **トピックモデル**：LSA（SVDを利用）、pLSA、LDAなど。文書集合内の潜在トピックを発見することにより文書を表現します。\n",
    "    \n",
    "    **固定表現を用いた単語ベクトル**：Word2Vec、FastText、GloVeなど。これらのモデルは、単語間の意味関係を捉えた単語の固定ベクトル表現を生成します。\n",
    "    \n",
    "    **動的表現を用いた単語ベクトル**：ELMo、GPT、BERTなど。これらのモデルは、文脈に応じて単語ベクトルを動的に調整し、語義や文脈の変化をより良く捉えます。\n",
    "\n",
    "    ここでは、固定表現を用いた単語ベクトルの主要なアルゴリズムとその不足点、および動的表現を用いた単語ベクトルの利点について主に比較します。\n",
    "\n",
    "2. 固定表現を用いた単語ベクトルアルゴリズムの比較：\n",
    "\n",
    "    2.1 word2vec VS fastText\n",
    "\n",
    "    共通点は、両者ともに単語の意味情報を捉えることができる点です。\n",
    "    \n",
    "    異なる点は、FastTextが単語の内部にあるサブワード情報を考慮することで未知語や語形変化を処理できるのに対し、Word2Vecは単語全体の表現に基づいています。\n",
    "\n",
    "    FastTextは有監督学習を利用したテキスト分類にも対応しています。-> [paper](https://aclanthology.org/E17-2068.pdf)\n",
    "\n",
    "    2.2 word2vec VS GloVe\n",
    "\n",
    "    Word2Vecは局所的な文脈を用いて単語の意味を捉えるため、類似の文脈に現れる単語間の細やかな意味の違いを捉えるのに優れています。また、計算効率が良いため、大規模なデータセットに対しても比較的高速に学習が可能です。GloVeは全体の共起統計に基づくため、単語間のグローバルな関係を捉えることができます。これにより、単語の共通の意味的特性を捉えるのに有効で、特に共起頻度が高い単語間の関係をよく表現できます。\n",
    "\n",
    "    word2vecは未知語やレアな単語に対してはそのベクトルを生成することができない。また、単語の意味が文脈によって変化する場合（多義語など）、その変化を捉えることが難しい。GloVeは単語の共起情報を事前に集計する必要があるため、そのプロセスが計算資源を多く消費します。また、局所的な文脈の変化に対する感度が低く、特定の文脈での単語の意味のニュアンスを捉えるのが若干難しい場合があります。\n",
    "\n",
    "    Word2Vecはローカルなコーパスを用いて訓練を行うため、新しいデータがある場合には迅速に学習することができます。一方で、GloVeは全体の共起行列を更新する必要があります。\n",
    "\n",
    "3. 固定表現を用いた単語ベクトルの欠点\n",
    "\n",
    "    \n",
    "    固定表現を用いた単語ベクトル：一語多義などの問題を解決できません。\n",
    "\n",
    "    => elmo、GPT、bert\n",
    "    \n",
    "    => METB: https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
