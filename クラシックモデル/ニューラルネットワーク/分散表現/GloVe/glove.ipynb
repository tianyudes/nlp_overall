{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. GloVeは何ですか。\n",
    "\n",
    "    一言で、GloVeは、大規模なテキストコーパス内での単語の共起情報を利用して、単語の意味的関係を捉えるベクトル表現を学習するためのアルゴリズムです。このプロセスでは、単語ペアの共起確率の対数と単語ベクトルのドット積との差を最小化することにより、似た意味を持つ単語が似たベクトル表現を持つように訓練されます。\n",
    "\n",
    "    目的関数：\n",
    "\n",
    "    ![objective_function](./image/GloVe_1.png)\n",
    "\n",
    "2. GloVeの主な特徴と利点\n",
    "    \n",
    "    2.1 GloVeの主な特徴と利点\n",
    "\n",
    "    共起行列: GloVeは、特定の「窓」サイズ内で単語がどのくらい頻繁に一緒に出現するかを計算することに基づいています。この情報は共起行列として組織され、その後の処理で単語ベクトルを生成するために使用されます。\n",
    "\n",
    "    グローバルな統計情報: GloVeは文書全体の統計情報を利用して単語の意味を捉えます。これにより、文脈上の微妙な違いをより良く捉えることができ、単語の埋め込みがより豊かな意味情報を含むようになります。\n",
    "\n",
    "    ベクトル演算: GloVeによって生成された単語ベクトルは、ベクトル演算を通じて意味的関係を表現できます。例えば、「king - man + woman」のような演算が「queen」に近いベクトルを生成するなど、単語間の類似性やアナロジー（類推）関係を捉えることができます。\n",
    "\n",
    "    スケーラビリティ: 大規模なコーパスに対しても効率的に処理を行うことができ、豊富な文脈情報を基にした高品質な単語ベクトルを生成します。\n",
    "\n",
    "    2.2 GloVeとWord2Vecの違い\n",
    "\n",
    "    一言で、GloVeは単語のグローバルな共起情報を用いて意味的類似性を捉えるのに対し、Word2Vecは単語の局所的な文脈情報を基にして単語の関連性を学習します。\n",
    "    \n",
    "    GloVeの観点\n",
    "    GloVeは、単語間のグローバルな共起関係を分析し、その共起頻度を基にして単語ベクトルを学習します。このプロセスでは、全コーパスを通じて単語がどのように共に現れるか（例えば、「犬」と「可愛い」、「猫」と「可愛い」の共起）を分析し、これらの統計的な情報を用いて単語間の意味的類似性をモデル化します。つまり、GloVeは「犬」と「猫」が似たような文脈で使われるというグローバルな情報から、これらの単語が意味的に似ていると結論づけます。\n",
    "\n",
    "    Word2Vecの観点\n",
    "    Word2Vec（特にSkip-Gramモデル）は、ある単語の周囲の単語（文脈）を予測することによって単語ベクトルを学習します。この場合、特定の単語（例えば「犬」）が与えられたときに、「可愛い」という単語が周囲に現れる確率を直接学習します。Word2Vecは、単語が出現する具体的な文脈に基づいて単語の意味を捉えますが、これにより「犬」と「猫」が似た文脈で使用されることから、結果としてこれらの単語のベクトルが似たようになります。\n",
    "\n",
    "    GloVeもWord2Vecも、結果的には「犬」と「猫」のように意味的に似た単語が似たベクトル表現を持つようになりますが、その過程は異なります。GloVeは単語の共起情報からグローバルな関係を、Word2Vecは個々の文脈から局所的な関係を捉えることで、単語の意味的類似性をモデル化します。どちらのモデルも、その訓練プロセスにおいて特定の単語間で意味的な類似性を「意図的に」作り出すわけではなく、単語がどのように使われるかに基づいて自然に類似性を学習しています。\n",
    "\n",
    "3. GloVe論文内容のまとめ\n",
    "\n",
    "    [論文リンク](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "\n",
    "    [Github](https://github.com/stanfordnlp/GloVe)\n",
    "\n",
    "    [core part](source_code/GloVe/GloVe.py)\n",
    "\n",
    "4. 参考資料\n",
    "    [高速かつ高性能な分散表現Gloveについて(PyTorch実装)](https://qiita.com/GushiSnow/items/e92ac2fea4f8448491ba)\n",
    "    [GloVe (Global Vectors)【分散表現】](https://cvml-expertguide.net/terms/nlp/glove/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "# サンプルのコーパスデータ\n",
    "sentences = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"barked\"],\n",
    "    [\"the\", \"cat\", \"meowed\"],\n",
    "    # ここにさらに文章を追加して、コーパスを充実させることができます\n",
    "]\n",
    "\n",
    "# GloVeコーパスの準備\n",
    "# 'Corpus()' は共起行列を構築するために使用されます\n",
    "corpus = Corpus()\n",
    "\n",
    "# コーパスから共起行列を生成\n",
    "# sentences: トレーニング用の文章データ\n",
    "corpus.fit(sentences, window=10)\n",
    "\n",
    "# GloVeモデルの初期化とトレーニング\n",
    "# no_components: 埋め込みベクトルの次元数\n",
    "# learning_rate: 学習率\n",
    "# epochs: トレーニングのエポック数\n",
    "# window: 共起行列を作成するためのウィンドウサイズ\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "\n",
    "# 単語 \"cat\" の埋め込みベクトルを取得\n",
    "cat_vector = glove.word_vectors[glove.dictionary[\"cat\"]]\n",
    "print(\"Cat vector:\", cat_vector)\n",
    "\n",
    "# モデルをファイルに保存\n",
    "glove.save(\"glove.model\")\n",
    "\n",
    "# 保存されたモデルをロード\n",
    "loaded_glove = Glove.load(\"glove.model\")\n",
    "\n",
    "# ロードされたモデルで再度単語 \"cat\" のベクトルを取得\n",
    "loaded_cat_vector = loaded_glove.word_vectors[loaded_glove.dictionary[\"cat\"]]\n",
    "print(\"Loaded Cat vector:\", loaded_cat_vector)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
