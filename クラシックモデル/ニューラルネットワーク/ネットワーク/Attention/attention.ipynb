{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Mechanism\n",
    "\n",
    "1. Seq2Seqについて\n",
    "\n",
    "    1.1 Seq2Seqの定義\n",
    "    Seq2Seqモデルは一連の入力を別の一連の出力に変換するために設計された深層学習のフレームワークです。基本的な構造は、エンコーダ部とデコーダ部の2つの主要なコンポーネントから構成されています。\n",
    "\n",
    "    1.2 seq2seqの構造\n",
    "        \n",
    "    1.2.1 encoder\n",
    "\n",
    "    エンコーダは、入力シーケンス（例えば、テキストの文章や単語の列）を受け取り、それを固定長のベクトル表現に変換します。このベクトルは、入力シーケンスの「意味」や「文脈」を捉えたもので、デコーダが出力シーケンスを生成する際のコンテキスト情報として機能します。エンコーダは通常、RNN、LSTM、GRUなどの再帰的ニューラルネットワークで構成されています。\n",
    "\n",
    "    \n",
    "\n",
    "    1.2.2 decoder\n",
    "\n",
    "    デコーダは、エンコーダから受け取った固定長のベクトルをもとに、ターゲットシーケンス（例えば、翻訳後の文章や要約文）を一つずつ生成していきます。デコーダもエンコーダと同様にRNN、LSTM、GRUなどで構成されることが多く、各ステップでの出力が次のステップの入力にも用いられます。デコーダは、エンコーダからの情報とこれまでに生成した出力を基に、次の単語（またはトークン）を予測します。\n",
    "    \n",
    "    ![seq2seq](./image/1-seq2seq.gif)\n",
    "    ![translation](./image/1-2-translation.gif)\n",
    "\n",
    "\n",
    "    1.3 seq2seqのディテール\n",
    "\n",
    "    緑色のエンコーダーは、入力シーケンスを処理し、入力情報を取得します。これらの情報は黄色いベクトル（コンテキストベクトルと呼ばれます）に変換されます。入力シーケンス全体を処理し終えた後、エンコーダーはコンテキストベクトルを紫色のデコーダーに送信します。デコーダーは、コンテキストベクトル内の情報を通じて、新しいシーケンスを一つずつ出力します。\n",
    "\n",
    "    ![seq2seq2](./image/1-3-encoder-decoder.gif)\n",
    "    ![seq2seq3](./image/1-3-mt.gif)\n",
    "   \n",
    "\n",
    "\n",
    "    ![seq2seq](./image/seq2seq.png)\n",
    "    ![seq2seq_example](./image/seq2seq_example.png)\n",
    "\n",
    "\n",
    "    1.5 Seq2Seqの欠点\n",
    "\n",
    "    Seq2Seqモデルにはいくつかの欠点があります。\n",
    "    \n",
    "    まず、入力シーケンスXの長さを無視しています。入力文が非常に長く、特に訓練セットの初期の文よりも長い場合、モデルの性能が急激に低下します。\n",
    "    \n",
    "    また、入力シーケンスXに対して区別をつけていません。入力Xを固定長でエンコードすると、文中の各単語に同じ重みを与えることになり、これはモデルの性能低下につながります。\n",
    "    \n",
    "    さらに、デコーダで最初の出力にエラーが発生すると、そのエラーが続くと修正されずに進行してしまいます。\n",
    "\n",
    "    1.6 source code example\n",
    "\n",
    "\n",
    "\n",
    "    1.7 その他\n",
    "\n",
    "    https://qiita.com/soldier-tn/items/28f2f03c3058d671de12\n",
    "    \n",
    "\n",
    "2. Attention\n",
    "\n",
    "    Attentionメカニズムは、ディープラーニングにおいて特に自然言語処理（NLP）や画像認識などの分野で重要な役割を果たしています。このメカニズムは、入力されたデータの中から重要な情報に「注意」を向け、その情報を強調することで、モデルがより効率的に学習し、予測を行うのを助けます。\n",
    "\n",
    "   ![attention](./image/1-7-attention.jpg)\n",
    "   \n",
    "    2.1 Attentionの原理\n",
    "\n",
    "    2.1.1 Seq2Seq with attentionの原理\n",
    "\n",
    "    ![attention](./image/2-attention.gif)\n",
    "    注意力メカニズムを追加した後、エンコーダーは最後の隠れ状態だけをデコーダーに渡すのではなく、すべての隠れ状態をデコーダーに渡します。デコーダーの各時点では、エンコーダーの隠れ状態とデコーダー現在の隠れ状態の間の相関性に基づいて、エンコーダーの異なる隠れ状態にスコアをつけています。\n",
    "\n",
    "    ![attention2](./image/2-2-attention.webp)\n",
    "    Attentionメカニズムでは、デコード時にデコーダーの隠れ状態とエンコーダーの全隠れ状態との間でスコアを計算し、その後このスコアに対してsoftmaxを適用します。そして、全隠れ状態をこのスコアで重み付けした後に加算し、コンテキストベクトルを得ます。\n",
    "\n",
    "    ![attention3](./image/2-3-attention.gif)\n",
    "    ステップ1から3でそれぞれ隠れ状態を一つずつ出力し、それらはh1、h2、h3となります。\n",
    "    \n",
    "    ステップ4では、まず\\<END>はデコーダーに入力し、その後隠れ状態h4を出力します。h4とh1、h2、h3との間の類似度をスコアとして計算し、このスコアを対応する隠れ状態と乗算してc4を得ます。c4とh4を結合した後、出力して「I」という単語を得ます。ステップ５では「I」という単語をデコーダーに入力して、同じく「am」という単語が得られます。\n",
    "\n",
    "\n",
    "    重みスコアの計算:\n",
    "    モデルは入力シーケンスの各要素（例えば、テキストの各単語や画像の各部分）の重要性を評価します。これは通常、入力とある参照点（例えば、クエリ）との間の関連性を計算することによって行われます。\n",
    "\n",
    "    ウェイトの割り当て:\n",
    "    各要素の重要性に基づいて、ウェイト（重み）が割り当てられます。より重要な情報には高いウェイトが、それほど重要でない情報には低いウェイトが割り当てられます。\n",
    "    \n",
    "    コンテキストベクトルの生成:\n",
    "    ウェイトと各要素の情報（ベクトル）を組み合わせて、コンテキストベクトルを生成します。このコンテキストベクトルは、特定のタスク（例えば、次の単語の予測、画像内の特定オブジェクトの認識）における入力の「要約」を表します。\n",
    "    \n",
    "    タスクの実行:\n",
    "    生成されたコンテキストベクトルを用いて、最終的なタスク（予測、分類など）が実行されます。\n",
    "\n",
    "    2.2 Attentionの種類\n",
    "\n",
    "    Self-Attention: 同じ入力シーケンス内の要素間の関連性を評価します。Transformerモデルで広く使用されています。\n",
    "\n",
    "    Cross-Attention: 異なる入力シーケンス間（例えば、翻訳タスクにおけるソーステキストとターゲットテキスト）の関連性を評価します。\n",
    "\n",
    "\n",
    "3. Chatbot　system by Seq2Seq and attention\n",
    "\n",
    "    Reference: https://github.com/khordoo/chatbot-pytorch\n",
    "\n",
    "    TODO: USe the https://github.com/khordoo/chatbot-pytorch dataset to build the system\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Addition:\n",
    "\n",
    "1. Teaching Forcing:\n",
    "    \n",
    "    Teacher Forcingは、特にシーケンス生成タスク（例えば、機械翻訳、テキスト生成など）で用いられる訓練技術です。この技術では、モデルの訓練中に、直前の正解の出力（または入力シーケンスの対応する要素）を次の時刻の入力として使用します。これにより、訓練中のモデルが正しいシーケンスを学習するのを助けることができます。Teacher Forcingを使用すると、各時点でのデコーダの入力として正解のトークンが供給されるため、訓練プロセスが安定し、加速します。\n",
    "\n",
    "    ただし、Teacher Forcingの使用は、訓練時と推論時（訓練後にモデルが新しい入力に対して出力を生成する過程）の間に「露出バイアス」（exposure bias）と呼ばれる問題を生じさせる可能性があります。訓練時には常に正しい前のトークンがデコーダに供給されますが、推論時にはモデル自身によって生成されたトークンを使用するため、訓練時に経験しなかった誤りが蓄積される可能性があります。これに対処するために、スケジュールドサンプリングなどのテクニックが提案されています。\n",
    "2. Curriculum learning\n",
    "\n",
    "    確率pを用いて、次の時点の入力として正解の出力y(t)を使うか、前の時点で生成された出力h(t)を使うかを決定します。通常、学習開始時にはpを大きくして早期収束を実現し、その後徐々にpを減少させます。\n",
    "\n",
    "3. beam search\n",
    "    \n",
    "    Beam Search（ビームサーチ）は、探索アルゴリズムの一種で、特に自然言語処理（NLP）の分野で、機械翻訳や音声認識などのシーケンス生成タスクにおいて広く利用されています。ビームサーチは、全ての可能性を探索する完全な探索に比べて、計算コストを大幅に削減しつつ、高品質な解を見つけ出すことを目的としています。\n",
    "\n",
    "    ビームサーチの特徴\n",
    "    制限された幅の探索: ビームサーチは、各探索ステップで保持する候補の数をビーム幅（ビームサイズとも呼ばれる）によって制限します。このビーム幅はアルゴリズムの性能と品質を決定する重要なパラメータです。\n",
    "\n",
    "    貪欲法に基づく: 各時点で、ビーム幅内で最も有望と考えられるいくつかの候補のみを選択し、残りの候補は切り捨てます。これにより、探索空間が大幅に削減されます。\n",
    "\n",
    "    近似解: ビームサーチは最適解を保証するものではありませんが、実用的には高品質な解を効率的に見つけることができます。\n",
    "\n",
    "    ビームサーチの適用例\n",
    "    機械翻訳: 翻訳候補のシーケンスを生成する際に、各ステップでビーム幅の数だけ最も可能性の高い部分シーケンスを選択し、最終的な翻訳候補として最もスコアの高いものを出力します。\n",
    "\n",
    "    音声認識: 音声信号からテキストへの変換を行う際に、ビームサーチを用いて最も可能性の高いテキストシーケンスを識別します。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
