{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RNN\n",
    "\n",
    "    1.1 RNNを設計する主な原因\n",
    "\n",
    "    シーケンスデータの処理\n",
    "    動機: 自然言語処理（NLP）、音声認識、時系列予測など、入力データが時間的な順序を持つタスクに対応するため。RNNは、これらのシーケンス内のパターンや依存関係を学習する能力があります。\n",
    "    \n",
    "    時間的な依存関係の把握\n",
    "    動機: 過去のデータが現在の出力に重要な影響を与える場合、RNNはそのような時間的な依存関係をモデル化できます。例えば、言語モデルでは、文の意味を理解するために前の単語の文脈が必要です。\n",
    "    \n",
    "    可変長の入力・出力\n",
    "    動機: 固定サイズの入力を扱う従来のニューラルネットワークと異なり、RNNは可変長の入力シーケンスを処理することができます。これにより、任意の長さのテキストや音声データを扱うことが可能になります。\n",
    "\n",
    "    1.2 考慮事項\n",
    "\n",
    "    長期依存性の問題\n",
    "    \n",
    "    課題: 標準的なRNNは、長期依存性の問題（勾配消失・勾配爆発）により、長いシーケンスデータでの学習が困難です。この問題を克服するために、LSTM（Long Short-Term Memory）やGRU（Gated Recurrent Unit）などの改良されたRNNが提案されています。\n",
    "\n",
    "    パラメータのチューニング\n",
    "    課題: RNNの学習率や隠れ層のユニット数などのハイパーパラメータの選択が、モデルの性能に大きく影響します。適切なハイパーパラメータを見つけるためには、多くの実験が必要になることがあります。\n",
    "    \n",
    "    計算コスト\n",
    "    課題: RNNはシーケンスの各時点で前の時点の隠れ状態を利用するため、並列計算が難しく、特に長いシーケンスを扱う場合には計算コストが高くなりがちです。\n",
    "    \n",
    "    過学習の防止\n",
    "    課題: RNNも他の深層学習モデルと同様に過学習（トレーニングデータに過剰に適合すること）のリスクがあります。これを防ぐために、ドロップアウトや正則\n",
    "\n",
    "2. LSTMとGRU\n",
    "\n",
    "    2.1 単純なRNNの欠点\n",
    "\n",
    "    単純なRNN（基本的な再帰的ニューラルネットワーク）は、理論上は時系列データの特徴を学習する強力な能力を持っていますが、実際には「長期依存性の問題」に直面します。この問題は、シーケンスが長くなるにつれて、初期の情報がネットワークを通過する際に失われることを意味します。具体的には、勾配消失や勾配爆発の問題が生じ、モデルが長期間の依存関係を効果的に学習することが難しくなります。\n",
    "\n",
    "    2.2 LSTM\n",
    "\n",
    "    LSTMは、入力ゲート、忘却ゲート、出力ゲートの3つの主要なゲートを持っています。これらのゲートは、それぞれ新しい情報を隠れ状態にどの程度追加するか、隠れ状態から情報をどの程度忘れるか、隠れ状態から次の層へとどの程度の情報を出力するかを制御します。LSTMは、多くの自然言語処理や音声認識タスクにおいて、高い性能を発揮します。\n",
    "\n",
    "    ![LSTM](./image/LSTM.png)\n",
    "\n",
    "    input gateは、セル状態を更新する役割を果たします。\n",
    "    \n",
    "    forget gateでは、通過させる情報と通化させない情報を決定します。\n",
    "    \n",
    "    output gateは、隠れユニットの値の更新方法を決定します。\n",
    "\n",
    "    ![LSTM_structure](./image/LSTM_structure.png)\n",
    "\n",
    "    ![LSTM_calculation](./image/LSTM_calculation.png)\n",
    "\n",
    "\n",
    "    2.3 GRU\n",
    "\n",
    "    GRUはLSTMを簡略化したバージョンであり、リセットゲートと更新ゲートの2つのゲートを使用します。これらは、新しい情報の統合方法と、過去の情報をどの程度保持するかを制御します。GRUはLSTMに比べてパラメータが少ないため、計算効率が良く、小さなデータセットでの学習が速くなる傾向があります。\n",
    "\n",
    "    ![GRU](./image/GRU.png)\n",
    "    ![GRU](./image/GRU_calculation.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# RNNモデルの定義\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # RNNレイヤーの定義\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        # 出力層の定義\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 初期隠れ状態をゼロに設定\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        \n",
    "        # RNN層の出力と隠れ状態を計算\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        # 最後のタイムステップの隠れ状態を出力層に通す\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "input_size = 1  # 入力特徴量の数\n",
    "hidden_size = 10  # 隠れ層のサイズ\n",
    "output_size = 1  # 出力特徴量の数\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "# モデル、損失関数、最適化手法の定義\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# トレーニングデータの準備（例として単純な数列）\n",
    "# 入力データのサイズは (バッチサイズ, シーケンス長, 特徴量の数)\n",
    "x_train = torch.tensor([\n",
    "    [[0.1], [0.2], [0.3], [0.4], [0.5]],\n",
    "    [[0.2], [0.3], [0.4], [0.5], [0.6]],\n",
    "    [[0.3], [0.4], [0.5], [0.6], [0.7]],\n",
    "    [[0.4], [0.5], [0.6], [0.7], [0.8]]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# 目標値\n",
    "y_train = torch.tensor([[0.6], [0.7], [0.8], [0.9]], dtype=torch.float32)\n",
    "\n",
    "# トレーニングループ\n",
    "for epoch in range(num_epochs):\n",
    "    # 順伝播\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # 逆伝播と最適化\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# テスト\n",
    "model.eval()  # 評価モードに切り替え\n",
    "with torch.no_grad():\n",
    "    test_input = torch.tensor([[[0.5], [0.6], [0.7], [0.8], [0.9]]], dtype=torch.float32)\n",
    "    test_output = model(test_input)\n",
    "    print(f'Test Input: {test_input.tolist()}')\n",
    "    print(f'Predicted Output: {test_output.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# LSTMモデルの定義\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size)  # 初期隠れ状態\n",
    "        c0 = torch.zeros(1, x.size(0), hidden_size)  # 初期セル状態\n",
    "        out, _ = self.lstm(x, (h0, c0))  # LSTM層の順伝播\n",
    "        out = self.fc(out[:, -1, :])  # 最後のタイムステップの出力\n",
    "        return out\n",
    "\n",
    "# モデルのインスタンス化とハイパーパラメータ設定\n",
    "input_size = 1\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "model = SimpleLSTM(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()  # 損失関数\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # 最適化手法\n",
    "\n",
    "# トレーニングデータの準備（例として単純な数列）\n",
    "x_train = torch.tensor([\n",
    "    [[0.1], [0.2], [0.3], [0.4], [0.5]],\n",
    "    [[0.2], [0.3], [0.4], [0.5], [0.6]],\n",
    "    [[0.3], [0.4], [0.5], [0.6], [0.7]],\n",
    "    [[0.4], [0.5], [0.6], [0.7], [0.8]]\n",
    "], dtype=torch.float32)\n",
    "y_train = torch.tensor([[0.6], [0.7], [0.8], [0.9]], dtype=torch.float32)\n",
    "\n",
    "# モデルのトレーニング\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# テスト\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = torch.tensor([[[0.5], [0.6], [0.7], [0.8], [0.9]]], dtype=torch.float32)\n",
    "    test_output = model(test_input)\n",
    "    print(f'Test Input: {test_input.tolist()}')\n",
    "    print(f'Predicted Output: {test_output.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# GRUモデルの定義\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size)  # 初期隠れ状態\n",
    "        out, _ = self.gru(x, h0)  # GRU層の順伝播\n",
    "        out = self.fc(out[:, -1, :])  # 最後のタイムステップの出力\n",
    "        return out\n",
    "\n",
    "# モデルのインスタンス化とハイパーパラメータ設定\n",
    "input_size = 1\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "model = SimpleGRU(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()  # 損失関数\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # 最適化手法\n",
    "\n",
    "# トレーニングデータの準備（例として単純な数列）\n",
    "x_train = torch.tensor([\n",
    "    [[0.1], [0.2], [0.3], [0.4], [0.5]],\n",
    "    [[0.2], [0.3], [0.4], [0.5], [0.6]],\n",
    "    [[0.3], [0.4], [0.5], [0.6], [0.7]],\n",
    "    [[0.4], [0.5], [0.6], [0.7], [0.8]]\n",
    "], dtype=torch.float32)\n",
    "y_train = torch.tensor([[0.6], [0.7], [0.8], [0.9]], dtype=torch.float32)\n",
    "\n",
    "# モデルのトレーニング\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# テスト\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = torch.tensor([[[0.5], [0.6], [0.7], [0.8], [0.9]]], dtype=torch.float32)\n",
    "    test_output = model(test_input)\n",
    "    print(f'Test Input: {test_input.tolist()}')\n",
    "    print(f'Predicted Output: {test_output.item():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
